{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "We will use the embeddings through the whole lab. They are simply represented by a weight matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_size = 4\n",
    "vocab_size = 10\n",
    "\n",
    "embedding = np.arange(embedding_size * vocab_size, dtype='float')\n",
    "embedding = embedding.reshape(vocab_size, embedding_size)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the embedding for a given symbol $i$, you may:\n",
    " - compute a one-hot encoding of $i$, then compute a dot product with the embedding matrix\n",
    " - simply index (slice) the embedding matrix by $i$, using numpy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 3\n",
    "onehot = np.zeros(10)\n",
    "onehot[i] = 1.\n",
    "onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_vector = np.dot(onehot, embedding)\n",
    "print(embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(embedding[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The Embedding layer in Keras\n",
    "\n",
    "In Keras, embeddings have an extra parameter, `input_length` which is typically used when having a sequence of symbols as input (think sequence of words). In our case, the length will always be 1.\n",
    "\n",
    "```py\n",
    "Embedding(output_dim=embedding_size, input_dim=vocab_size,\n",
    "          input_length=sequence_length, name='my_embedding')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import keras\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    output_dim=embedding_size, input_dim=vocab_size,\n",
    "    input_length=1, name='my_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use it as part of a Keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "x = Input(shape=[1], name='input')\n",
    "embedding = embedding_layer(x)\n",
    "model = Model(inputs=x, outputs=embedding)\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding weights are randomly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict([[0],\n",
    "               [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of an embedding layer is then a 3-d tensor of shape `(batch_size, sequence_length, embedding_size)`\n",
    "To remove the sequence dimension, useless in our case, we use the `Flatten()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "\n",
    "x = Input(shape=[1], name='input')\n",
    "\n",
    "# Add a flatten layer to remove useless \"sequence\" dimension\n",
    "y = Flatten()(embedding_layer(x))\n",
    "\n",
    "model2 = Model(inputs=x, outputs=y)\n",
    "model2.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.predict([[0],\n",
    "                [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we re-used the same `embedding_layer` instance in both `model` and `model2`: therefore the two model share exactly the same weights in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
