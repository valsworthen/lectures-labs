{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "We will use the embeddings through the whole lab. They are simply represented by a weight matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   1.   2.   3.]\n",
      " [  4.   5.   6.   7.]\n",
      " [  8.   9.  10.  11.]\n",
      " [ 12.  13.  14.  15.]\n",
      " [ 16.  17.  18.  19.]\n",
      " [ 20.  21.  22.  23.]\n",
      " [ 24.  25.  26.  27.]\n",
      " [ 28.  29.  30.  31.]\n",
      " [ 32.  33.  34.  35.]\n",
      " [ 36.  37.  38.  39.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_size = 4\n",
    "vocab_size = 10\n",
    "\n",
    "embedding = np.arange(embedding_size * vocab_size, dtype='float')\n",
    "embedding = embedding.reshape(vocab_size, embedding_size)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To access the embedding for a given symbol $i$, you may:\n",
    " - compute a one-hot encoding of $i$, then compute a dot product with the embedding matrix\n",
    " - simply index (slice) the embedding matrix by $i$, using numpy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "onehot = np.zeros(10)\n",
    "onehot[i] = 1.\n",
    "onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 12.  13.  14.  15.]\n"
     ]
    }
   ],
   "source": [
    "embedding_vector = np.dot(onehot, embedding)\n",
    "print(embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 12.  13.  14.  15.]\n"
     ]
    }
   ],
   "source": [
    "print(embedding[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The Embedding layer in Keras\n",
    "\n",
    "In Keras, embeddings have an extra parameter, `input_length` which is typically used when having a sequence of symbols as input (think sequence of words). In our case, the length will always be 1.\n",
    "\n",
    "```py\n",
    "Embedding(output_dim=embedding_size, input_dim=vocab_size,\n",
    "          input_length=sequence_length, name='my_embedding')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    output_dim=embedding_size, input_dim=vocab_size,\n",
    "    input_length=1, name='my_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's use it as part of a Keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "x = Input(shape=[1], name='input')\n",
    "embedding = embedding_layer(x)\n",
    "model = Model(input=x, output=embedding)\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The embedding weights are randomly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.01936201,  0.01061306, -0.00502486,  0.02421001],\n",
       "        [-0.0071108 , -0.00054221, -0.00888319, -0.01043938],\n",
       "        [ 0.02423747, -0.03029217,  0.04300025,  0.04906172],\n",
       "        [ 0.02151183, -0.03897278, -0.02845473,  0.03577714],\n",
       "        [ 0.02908627, -0.00221761,  0.04954894, -0.0006169 ],\n",
       "        [-0.04881933, -0.00207155,  0.01976109, -0.03441812],\n",
       "        [-0.04364808, -0.03180511,  0.00863915,  0.0453595 ],\n",
       "        [ 0.02288247, -0.00607735, -0.04348791, -0.03095592],\n",
       "        [-0.03011045,  0.02573893, -0.01666873, -0.0387949 ],\n",
       "        [-0.04212544,  0.00182465, -0.00680971,  0.04644271]], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.01936201,  0.01061306, -0.00502486,  0.02421001]],\n",
       "\n",
       "       [[ 0.02151183, -0.03897278, -0.02845473,  0.03577714]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[0],\n",
    "               [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The output of an embedding layer is then a 3-d tensor of shape `(batch_size, sequence_length, embedding_size)`\n",
    "To remove the sequence dimension, useless in our case, we use the `Flatten()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Flatten\n",
    "\n",
    "x = Input(shape=[1], name='input')\n",
    "\n",
    "# Add a flatten layer to remove useless \"sequence\" dimension\n",
    "y = Flatten()(embedding_layer(x))\n",
    "\n",
    "model2 = Model(input=x, output=y)\n",
    "model2.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01936201,  0.01061306, -0.00502486,  0.02421001],\n",
       "       [ 0.02151183, -0.03897278, -0.02845473,  0.03577714]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict([[0],\n",
    "                [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that we re-used the same `embedding_layer` instance in both `model` and `model2`: therefore the two model share exactly the same weights in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
